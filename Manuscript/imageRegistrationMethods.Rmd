
# Image registration with deep learning

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{Figures/overview.pdf}
\caption{Graphical overview of the works reviewed including (a) publications per year, (b)
         choice of neural network API, (c) anatomy (where applicable), and (d) publishing
        venue.
        }
\label{fig:overview}
\end{figure}

The following overview of deep learning image registration methods is loosely categorized
based on the discussion of network architectures given in the previous section.
Specifically, we first discuss early work in which transformations were derived
from CNN-based identification and localization of corresponding features in
image pairs.  We then review "two channel" approaches in which fixed and moving images
are concatenated channelwise in the input layer.  This leads to an overview of
methods involving the related siamese and pseudo-siamese architectures.  The final
category concerns those adverserial approaches employing GANs.  Other methods which
do not fit in any of the above categories are also included.  Each method is listed
in Table 1 with a graphical summary provided in Figure \ref{fig:overview}.

\input{methodsTable.tex}

## Image registration via feature localization

Much of the early work incorporating deep learning into solving image
registration problems involved the detection of corresponding features
and then using that information to determine the correspondence relationship
between the fixed and moving image pair.  For example, just at the start of the
current era of deep learning in image-related research, [@Sergeev:2012aa]
proposed point correspondence detection using multiple feed-forward neural
networks each of which is trained to detect a single feature.  These
neural networks are relatively simple consisting of two hidden layers each
with 60 neurons where the output is a probability of it containing a specific
feature at the center of a small image neighborhood.  These detected point correspondences
are then used to estimate the total affine transformation using the RANSAC
algorithm [@Fischler:1981aa].
Similarly, DeepFlow [@Weinzaepfel:2013aa] uses CNNs to detect
matching features (i.e., _deep matching_) which are then used as
additional information in the large displacement optical flow framework
[@Brox:2011aa].  A relatively small architecture is employed consisting
of six layers and used to detect features at different convolution sizes
and then matched across scales.

A similarity measure for multimodal registration is formulated in terms
of CNNs in the work of [@Simonovsky:2016aa].  A two channel network is
developed for input image patches (T1- and T2-weighted brain images).
A B-spline image registration algorithm developed from the Insight Toolkit
is used to leverage the output CNN-based similarity measure for comparison
with an identical registration set-up employing mutual information.
Finally, in the category of feature learning, Wu et al. use stacked auto-encoders
(SAE) to map patchwise image content to learned feature vectors [@Wu:2016aa].  These
patches are then subsampled based on the importance criteria outlined in
[@Wang:2010aa] which tends towards regions of high informational content
such as edges. The SAE-based feature vectors at these image patches are then
used to drive a HAMMER-based registration [@Shen:2002aa] which is inherently
a feature-based, traditional image registration approach.

## Two channel architectures for image registration

### Voxelmorph

Voxelmorph was first introduced in [@Balakrishnan:2018aa] which incorporates
a U-net architecture with spatial transformer network.  The input layer
consists of the concatenated full fixed and moving image volumes resized
and cropped to $160 \times 192 \times 224$ voxels. The output is the voxelwise
displacement field of the same size as the input (times three for each vector
component).  The loss function for training combines cross correlation and
regularization.  This was significantly updated in [@Dalca:2018aa] to
utilize stationary velocity fields for generating diffeomorphic transforms
which employed novel scaling and squaring network layers.  The underlying
code has been made available[^V] which has facilitated independent evaluations
such as [@Nazib:2018aa] to compare performance with traditional algorithms (i.e.,
IRTK [@Rueckert:1999aa], AIR [@Woods:1993aa], Elastix [@Klein:2010aa],
ANTs [@Avants:2011aa], and NiftyReg [@Modat:2010aa]).

[^V]: https://github.com/voxelmorph/voxelmorph

### Homography estimation

Two algorithms for more traditional computer vision applications
are proposed in [@DeTone:2016aa] and [@Nguyen:2018aa] where both
are based on the VGG architecture [@Simonyan:2014] for 2-D homography
estimation.  The former framework includes both a regression network
for determining corner correspondence and a classification network
for providing confidence estimates of those predictions.  The work
in [@Nguyen:2018aa], which is publicly available[^H1], uses patch
pairs as input and the L1 photometric loss between them to remove
the need for direct supervision.  A spatial transform layer is
also adapted for homography transformations.

[^H1]: https://github.com/tynguyen/unsupervisedDeepHomographyRAL2018

### Training loss on ground truth transformations

Instead of training with a loss function based on similarity measures between
fixed and moving images, the works of [@Rohe:2017aa;@Eppenhof:2018aa] formulate
the loss in terms of the squared difference between ground-truth and predicted
transformation parameters.  In terms of network architecture, [@Rohe:2017aa]
employs a variant of U-net for training/prediction based on
reference deformations provided by registration of previously segmented
ROIs for cardiac matching where priority is alignment of the
epicardium and endocardium.  Displacement fields are parameterized by stationary
velocity fields.  In contrast, [@Eppenhof:2018aa] uses a smaller version of
the VGG architecture to learn the $6 \times 6 \times 6$ thin-plate spline grid.

### Training loss on similarity metrics

Intermodality transformations involving CT and MRI are learned by training on the
intramodality image pairs in [@Cao:2017aa].  The basic U-net architecture
using input patches of size $68 \times 68 \times 68$
incorporates a loss function combining normalized cross correlation (NCC) and
explicit regularization.  A related idea is developed in [@Hu:2018aa] which
uses labeled data and intensity information during the training phase such
that only unlabeled image data is required for prediction.  The architecture
is a densely connected U-net architecture with three types of residual
shortcuts.  They also use a
multiscale Dice function with an explicit regularization term in the loss
function for estimating both global and local transformations.


## Siamese and pseudo-siamese architectures for image registration

### Geodesic shooting with Quicksilver

The large deformation diffeomorphic metric mappings (LDDMM) framework for image matching
derives from the theoretical foundations underlying diffeomorphic *flows*
[@Trouve:1995aa;@Christensen:1996aa;@Dupuis:1998].  Such diffeomorphisms are sufficiently
differentiable bijective mappings, or transformations, which have sufficiently
differentiable inverses. Specifically, the set of possible
diffeomorphic mappings, $\phi(\mathbf{x}, t)$ ($\mathbf{x} \in \Omega$, $t \in [0,1]$),
between two images, $I$ and $J$ can be described as the
collection of *paths* connecting the two images on a manifold determined by
the equation

\begin{equation}
\int_{0}^{1} \|v(t)\|^2_L dt + \int_{\Omega} | I \circ \phi^{-1}(x,1) - J|^2 d\Omega.
\label{eq:lddmm}
\end{equation}

$v$ is a time-dependent smooth field dictated by the functional norm $L$  and determines
the mapping via the ordinary differential equation

\begin{equation}
\frac{d\phi(\mathbf{x},t)}{dt} = v( \phi(\mathbf{x},t), t), \phi( \mathbf{x}, 0) = \mathbf{Id}.
\end{equation}

The optimal diffeomorphic transformation  between $I$ and $J$ can be described
as a geodesic [@Beg:2005aa] connecting the two images.  Traditionally,
computational approaches to determining
this geodesic path involve discretization of the velocity field followed by numerical
integration.  This is performed for a given number of iterations where, presumably,
convergence implies arrival at this geodesic (i.e., optimal) path.  Alternatively, based on
the work of [@Miller:2006aa], the Euler-Lagrange equations for Equation (\ref{eq:lddmm})
can be written as a system incorporating a "momentum" term.  It was further demonstrated
that the initial momentum determined the entire geodesic path.  This alternative perspective
engendered a new approach to determining the diffeomorphic solution between two images,
known as _geodesic shooting_ (e.g., [@Beg:2005aa;@Vialard:2012aa]).  Although initially
formulated in terms of scalar momenta [@Vialard:2012aa], a vector formulation was proposed
in [@Singh:2013aa] which tends towards superior numerical behavior.

The supervised deep learning technique of Yang et al. [@Yang:2017aa], known as _Quicksilver_,
leverages this geodesic shooting/vector momentum optimization approach for determining
optimal diffeomorphic transformations.  The network architecture consists of two parallel encoders
for separate fixed/moving image patches ($15 \times 15 \times 15$ voxels)
feature learning.  The output is then concatenated and sent
through three identical decoder branches (one for each dimension) which comprises the
inverse operations as the single encoder branch.  Thus, the output consists of the predicted
vector momentum map which, as described above, determines the total transformation.  In
order to improve accuracy of the predicted momentum maps, a follow-on correction network
is also proposed.  This correction network, trained by inverting the mapping produced
by the predicted momentum and computing the residual error, is meant to account for
large deformations across patch boundaries.  Of note, Quicksilver, written in PyTorch [@paszke:2017aa],
is one of the handful of algorithms surveyed which has been made publicly available[^Q].

[^Q]: https://github.com/rkwitt/quicksilver


## Adverserial image registration approaches

In order to constrain the mapping between moving and fixed images,
the GAN-based approach outlined in [@Mahapatra:2018aa] combines
a content loss term (which includes subterms for normalized mutual
information, structural similarity [@Wang:2004aa], and a VGG-based
filter feature L2-norm between the two images) with a "cyclical" adverserial
loss.  This is constructed in the style
of [@Zhu:2017aa] who proposed this GAN extension, viz., CycleGAN,
to ensure that
the normally underconstrained forward intensity mapping is consistent with
a similarly generated inverse mapping for "image-to-image translation"
(e.g., converting a Monet painting to a realistic photo or rendering a
winter nature scene as its summer analog).  However, in this case, the
cyclical aspect is to ensure a regularized field through forward and
inverse displacement consistency.

The work of [@Hu:2018ac] employs discriminator training between finite-element
modeling and generated displacements for the prostate and surrounding tissues
to regularize the predicted displacement fields.  The generator loss employs
the weakly supervised learning method proposed by the same authors in [@Hu:2018ab]
whereby anatomical labels are used to drive registration during training only.
The generator is constructed from an encoder/decoder architecture based on
ResNet blocks [@He:2015].
The prediction framework includes both localized tissue deformation and the
linear coordinate-system-changes assocated with the ultrasound imaging
acquisition.

In [@Fan:2018aa], the discriminator loss is based on quantification of how
well two images are aligned where the negative case derives from the registration
generator
and the positive cases consist of identical images (plus small perturbations).
Explicit regularization is added to the total loss for the registration network.
The registration network consists of a U-net type architecture which takes two
3-D image patches from the image pair as input and produces a patchwise
displacement field.  The discriminator network takes an image pair as input
and outputs the similarity probability.


