
# Preliminaries

Prior to describing the various image registration algorithms that have been recently
proposed in the literature which incorporate elements of deep learning, we first describe some basic
architectural components specifically relevant to such a discussion which include:

* convolutional neural networks,

* spatial transformer networks,

* diffeomorphic transformer networks, and

* siamese networks.

\textcolor{red}{Should we discuss the following?}

* \textcolor{red}{Deformable convolutional networks} [@Dai:2017aa]

Since all but a small subset of components can be included for discussion, we defer the interested
reader to the thorough cited earlier in addition to pertinent textbooks (e.g.,
[@Goodfellow:2016aa]) for additional information.

## Convolutional neural networks

The grid-like informational content of certain data structures, such as 2-D and 3-D images, is
perfectly suited to CNN-based training.  The major elements of CNNs are
localized convolutions, connections, and pooling [@LeCun:2015aa].
As indicated by its name, the distinguishing characteristic
of CNNs is the use of convolution instead of matrix operations in one or more of its constituent
layers [@Goodfellow:2016aa] where the output are feature maps.  These feature maps are typically
generated in an hierarchical fashion synthesizing simple
geometric features at the base convolutional layers (lines, corners, etc.) progressing to more
abstract features at the apical layers.
The localized connections and weight-sharing provide a form of regularization while
simultaneously reducing memory requirements [@Goodfellow:2016aa].  The size of the
convolution kernel, known as the "receptive field," determines the degree of localized
connections.  Finally, the accompanying pooling layers are used to subsample the
convolutional feature maps in a way that statistically summarizes voxel neighborhoods
within the feature maps.  An illustration of a bare-bones CNN configuration is provided in
Figure \ref{fig:convnet} which depicts
the core components of convolution and max pooling.  Architectural novelty derives from
innovative arrangements of these core (and other) network components and the connections between
them.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{Figures/ConvNet.pdf}
\caption{The basic elements of the CNN.
        The convolutional layer comprises several filters which
        are optimized in terms of their responses to various features
        found in the input layer.  Pooling is used to extract salient
        features and reduce computational complexity and passed on to
        subsequent layers.
        }
\label{fig:convnet}
\end{figure}

## Siamese networks

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{Figures/SiameseNet.jpg}
\caption{Diagrammatic illustration of the spatial transformer network.
        }
\label{fig:stn}
\end{figure}

## Spatial transformer networks

In 2015 Jaderberg and his fellow co-authors described a powerful new module, known
as the spatial transformer network (STN) [@Jaderberg:2015aa] which figures prominently
in many of the image registration approaches that we review below.
Generally, STNs enhance CNNs by permiting a flexibility which allows for an explicit spatial
invariance that goes beyond the implicitly limited translational invariance associated
with the architecture's pooling layers.  In many image-based tasks
(e.g., localization or segmentation), designing an
algorithm that can account for possible pose or geometric variation of the
object(s) of interest within the image is crucial for maximizing performance.
The STN is a fully differentiable layer which can be inserted anywhere in the
CNN to learn the parameters of the transformation of the input feature map (not
necessarily an image) which renders the output in such a way to optimize the
network based on the specified loss function.  The added flexibility and the
fact that there is no manual supervision or special handling required makes
this module an essential addition for any CNN-based toolkit.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{Figures/STN.pdf}
\caption{Diagrammatic illustration of the spatial transformer network.  The STN
         can be placed anywhere within a CNN to provide spatial invariance for the
         input feature map.  Core components include the localization network used
         to learn/predict the parameters which transform the input feature map.
         The transformed output feature map is generated with the grid generator
         and sampler.
        }
\label{fig:stn}
\end{figure}

An STN comprises three principal components:  1) a localization network,
2) a grid generator, and 3) a sampler (see Figure \ref{fig:stn}).  The localization
network uses the input feature map to learn/regress the transformation parameters
which optimize a specified loss function.  In many examples provided, this amounts
to transforming the input feature map to a quasi-canonical configuration to facilitate,
for example, classification.  The actual architecture of the localization network is
fairly flexible and any conventional architecture, such as a fully connected network
(FCN), is suitable as long as the output maps to the continuous estimate of the
transformation parameters.  These transformation parameters are then applied to
the output of the grid generator which are simply the regular coordinates of the input
image (or some normalized version thereof).  The sampler, or interpolator, is used
to map the transformed input feature map to the coordinates of the output feature map.

Since Jaderberg's original STN formulation, extensions have been proposed such as the
inverse compositional STN (IC-STN) [@Lin:2017aa] and the diffeomorphic
transformer network [@Detlefsen:2018aa].  We defer discussion of the latter to the
next subsection but briefly describe the former.  Two issues with STN include:
1) potential boundary effects in which learned transforms require sampling outside
the boundary of the input image which can cause potential learning errors for subsequent
layers and 2) the single-shot estimate of the learned transform which
can compromise accuracy for large transformation distances.  The IC-STN address
both of these issues by 1) propagating transformation parameters instead of
propagating warped input feature maps until the final transformation layer and
2) recurrent usage of the localization network for inferring transform
compositions in the spirt of the inverse compositional Lucas-Kanade algorithm [@Baker2004].


## Deep diffeomorphic transformer networks

Although discussion of transform generalizability was included in the original STN paper
[@Jaderberg:2015aa], discussion was limited to affine, attention (scaling + translation),
and thin-plate spline transforms which all fill the requirements of differentiability.  This
work was extended to diffeomorphic transforms in [@Detlefsen:2018aa].  The
computational load associated with generating traditional diffeomorphisms through
velocity field integration [@Beg:2005aa] motivated the use of continuous piecewise
affine-based (CPAB) transformations [@Freifeld:2017aa].  The CPAB approach utilizes a
tesselation of the image domain which translates into faster and more accurate generation
of the resulting diffeomorphism.  Although this does constrain the flexibility of the
final transformation, the framework provides an efficient compromise for use in deep
learning architectures.  Analogous to traditional image registration,  the
deep diffeomorphic transformer layer can be placed in serial following an affine-based STN layer
for a global-to-local total transformation estimation.  This is demonstrated
in the experiments reported in [@Detlefsen:2018aa].
Similar to the many publicly available implementations of STN, the authors provide
their own Tensorflow implementation of the diffeomorphic transformer network.[^P1]
The authors employ CUDA-based calculations for evaluating the CPAB gradients and
transforms due to speed considerations.

[^P1]: https://github.com/SkafteNicki/ddtn

## Enhancing CNNs with CoordConv

Although not discussed let alone used in any of the papers reviewed below, the insight provided
in [@Liu:2018aa] deserves consideration due to the subject matter of encoding spatial coordinates
in CNN layers and its relevance to image registration.  The authors describe a perplexing issue
encountered during the course of their research.
Reducing the core issue to toy examples, the authors demonstrate that training CNNs to
regress cartesian coordinates from sparse, feature map pixel encodings
(and vice versa) is highly problematic for conventional CNNs.  In order to remedy this
deficiency, the authors propose _CoordConv_ which involves the modification
of the conventional CNN layer with the concatenation of additional coordinate channels
to the input.  By explicitly encoding spatial information at each grid point in the
input layer of the CNN, the authors improve performance not only in the toy examples
but also in detection with the MNIST data set and in reinforcement learning scenarios
involving video game play.  Although not explicitly tested in the image registration
problem domain, it is possible that such straightforward modifications to
current architectures would substantially improve performance.


