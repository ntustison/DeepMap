
# Introduction

Determining the spatial correspondence between imaging domains is frequently a
critical component in quantitative image analysis workflows.  The trajectory of
image registration theoretical and technological development has led to increasingly
high quality transformational mappings that have significantly improved performance in related
processing tasks (e.g., image segmentation via joint label fusion [@Iglesias:2015aa])
and imaging-based statistical analysis involving template-based normalization
(e.g., voxel-based morphometry [@Ashburner:2000aa] and sparse canonical correlation analysis [@Avants:2010aa]).  Several reviews
[@Brown:1992;@Maintz:1998aa;@Pluim:2003aa;@Gholipour:2007aa;@Sotiras:2013aa;@Viergever:2016aa;@Keszei:2017aa]
have charted this chronology and provided insight into related issues such as algorithmic
classification, available implementations, evaluation strategies and speculation
concerning possible future directions of the field.  While prescient in many respects,
such speculation vis-à-vis the  resurgence of deep learning is understandably
limited due to its recent explosion in popularity and research focus.


The foundational concepts that form the basis for contemporary deep learning research
dates back decades (e.g., [@Ivakhnenko:1971aa]).  Since this early seminal work,
major developmental milestones include the *Neocognitron*, an early neural network
for character recognition [@Fukushima:1980aa], and convolutional neural networks
("CNNs" or "ConvNets") utilized in speech [@Waibel:1987aa] and visual signal processing
[@LeCun:1989aa], largely inspired by the visual cell types of the feline visual cortex
[@Hubel:1962aa].  Historical neural networks are differentiated from their modern progeny
by the deep, or "hidden," layering that characterizes current architectures and is one of the
principal reasons for the
extreme performance gains seen in the contemporary literature.  The training of modern architectures is
made computationally tractable with innovations such as gradient-based optimization using backpropagation
(first performed in [@LeCun:1989aa]) and advances in hardware [@LeCun:2015aa].
Uptake by both industry and academia alike is further facilitated through various
neural network open-source platforms (e.g., Tensorflow [@tensorflow] and Keras [@keras]).

A key event in the widespread adoption of CNNs was the 2012
ImageNet Large Scale Visual Recognition Challenge for object classification [@Russakovsky:2015aa].
The winning entry, a CNN-based architecture colloquially known as *AlexNet* [@Krizhevsky:2012],
reduced the error rate by almost half over other entries.  Subsequent years' competitions were
dominated by CNN variants such as VGG [@Simonyan:2014], GoogLeNet [@Szegedy:2015] and
ResNet [@He:2015] with performance ultimately exceeding human performance in 2015
[@He:2015ab].  Additional competition outlets, including conference-based venues
(e.g., NeurIPS[^0]) and community-based platforms, such as Kaggle[^1], continue to highlight
the utility of CNNs as comprehensive approaches to computer vision problems.  This is in addition
to the sheer number of formal research reports discussed in the same conferences, published
in dedicated journals and hosted in online technical repositories.  Notable reviews
by key figures in the field include those of
Yann LeCun, Yoshua Bengio, Geoffrey Hinton [@LeCun:2015aa], and Jürgen Schmidhuber
[@Schmidhuber:2015aa].

[^0]: https://nips.cc

[^1]: Following the 2017 ImageNet challenge, in which the vast majority of
teams surpassed the 5\% classification error rate threshold, the ImageNet organizers ceded management
to the Kaggle community which maintains a running performance assessment in ostensible perpetuity
[@imageNetKaggle].

Early CNN-based research tailored to medical imaging dates back to the 1990s with classification
tasks providing the majority of use cases (e.g., lung nodule classification
[@Lo:1992aa;@Lo:1993aa] and breast tissue differentiation [@Chan:1995aa;@Sahiner:1996aa]).
Despite the early adoption by certain research groups, widespread uptake did not occur
until much later.  Several deep learning overviews specific to medical imaging
have been presented in the recent literature

* in editorial form [@Greenspan:2016aa],

* specific to generative adverserial networks (GANs) [@Yi:2018aa],

* related to MRI [@Mazurowski:2018aa] with specific focus on neuro applications [@Bernal:2018aa],

* for issues related to radiation therapy [@Sahiner:2018aa],

* surveying various medical imaging applications [@Ker:2018aa], and

* as general reviews [@Suzuki:2017aa;@Shen:2017aa;@Litjens:2017aa;@Anwar:2018aa;@Biswas:2019aa;@Altaf:2019aa].

Despite the thorough treatment of the topic contained in these reviews, discussion of chronological
adoption within the community is limited.  Regardless,
one can informally gauge this evolution from utilization of alternative
machine learning techniques to predominately CNN-based approaches from the various
competitions held simultaneously with medical imaging conferences.  For example,
the annual Multimodal Brain Tumor Segmentation (BraTS) Challenge has taken
place under the auspices of the International Conference on Medical Image Computing
and Computer Assisted Intervention (MICCAI) since 2012 wherein large sets of training
data are provided to the competitors who attempt to perform a voxelwise labeling
of the constituent components of brain tumors from multimodal MR image data.  The winning
entries from the first two years employed random forest classifiers for segmentation [@Menze:2015aa].
Although variations of the traditional random forest scheme continued to be well represented
in the 2014 Challenge, CNN-based image segmentation algorithms made an appearance [@brats2014].
By 2018, CNN-based pipelines were, by far, the most common [@brats2018]
with specific preference being that of the U-net architecture [@Ronneberger:2015aa;@Falk:2019aa]
which, as we describe below, features prominently in deep learning-based
image registration.

Conspicuously, coverage of the topic of deep learning-based image registration,
relative to the related algorithmic categories of image classification and segmentation,
has not been as extensive in the reviews mentioned above, despite its prominence in
the broader research literature.  This disparity seems to be similarly
reflected in the quantity of published research for those respective categories
[@Litjens:2017aa;@Yi:2018aa].  This review is meant to address this disparity and
thus provide an overview of the current state-of-the-art of this burgeoning
subfield.

\textcolor{blue}{
Given the prospective readership of this review, we only briefly sketch
background material traditionally associated with image registration.
This permits a follow-up introduction to generic differences (traditional
vs. deep learning) including supervised vs. unsupervised optimization
and various key network elements
that characterize certain deep learning-based image registration architectures.
Within this
subsection, we also include additional network innovations which might
find utility in future architectures.  Based on these considerations
of readership and topical familiarity, we organize discussion of current
techniques based on an architectural classification (versus a more traditional categorization
based on, for example, similarity metric).  Finally, we briefly editorialize concerning
the present and continued future confluence of deep learning and image registration.}

