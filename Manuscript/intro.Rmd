
# Introduction

Determining the spatial correspondence between imaging domains is frequently a
critical component in quantitative image analysis workflows.  The evolution of
image registration theoretical and technological development has led to increasingly
high quality transformational mappings that have significantly improved performance in related
processing tasks (e.g., image segmentation via joint label fusion [@Iglesias:2015aa])
and imaging-based statistical analysis strategies (e.g., sparse canonical correlation
analysis [@Avants:2010aa]).  Several reviews
[@Brown:1992;@Maintz:1998aa;@Pluim:2003aa;@Gholipour:2007aa;@Viergever:2016aa;@Keszei:2017aa]
have charted this chronology and provided insight into related issues such as algorithmic
classification, available implementations, evaluation strategies, and speculation
concerning future directions of the field.  While prescient in many respects,
speculation vis-Ã -vis deep learning was somewhat limited due to deep learning's relatively recent
and sudden explosion in popularity and research focus.

The foundational concepts that form the basis for contemporary deep learning studies
date back decades [@LeCun:2015aa;@Ivakhnenko:1971aa].  From this historically seminal work,
major developmental milestones include the *Neocognitron*, an early neural network architecture
for character recognition [@Fukushima:1980aa], and convolutional neural networks
(CNN or ConvNets) utilized in speech [@Waibel:1987aa] and visual signal processing
[@LeCun:1989aa], largely inspired by the visual cell types of the feline visual cortex
[@Hubel:1962aa].  The common approach to gradient-based optimization of CNNs
using backpropagation was first performed in [@LeCun:1989aa].  A key event in the
widespread adoption of CNNs was the 2012
ImageNet Large Scale Visual Recognition Challenge for object classification [@Russakovsky:2015aa].
The winning entry, an architecture colloquially known as *AlexNet* [@Krizhevsky:2012],
reduced the error rate by almost half over other entries.  Subsequent years' competitions were
dominated by CNN variants such as VGG [@Simonyan:2014] and GoogLeNet [@Szegedy:2015].

\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/ConvNet.pdf}
  \caption{The basic elements of the convolutional neural network.  Following the input layer, }
\end{figure}



* Local connections
* shared weights
* pooling
* use of many layers


<!--


* "deep" refers to many "hidden" layers

* vanishing gradient problem (Hochreiter, S.; et al. (15 January 2001). "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies". In Kolen, John F.; Kremer, Stefan C. A Field Guide to Dynamical Recurrent Networks. John Wiley & Sons. ISBN 978-0-7803-5369-5.)

* backpropagation algorithm


* "There are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers." (LeCun 2015)



* Brief history of deep learning.  Cite the various review articles.

* Available resources

    * The usual packages (tensorflow, theano, Caffe, Lasagne, Flux (Julia), Keras (python and R))

    * NiftyNet

    * ANTsRNet

    * Countless repositories of individual implementations

* Brief history of image registration

* Structure of the review

    * Discussion --- what are some of the challengs specific to image registration?

-->